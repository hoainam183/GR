import re
import json
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class Chunk:
    """Äáº¡i diá»‡n cho má»™t chunk dá»¯ liá»‡u"""

    id: str
    text: str
    metadata: Dict

    def to_dict(self):
        return asdict(self)


class QuyCheDaoTaoExtractor:
    """Extract vÃ  chunk dá»¯ liá»‡u tá»« Quy cháº¿ Ä‘Ã o táº¡o ÄHBK"""

    def __init__(self, text: str):
        self.text = text
        self.chunks = []

    def extract_chuong_mapping(self) -> Dict[int, str]:
        """Mapping tá»« sá»‘ Äiá»u sang ChÆ°Æ¡ng"""
        return {
            range(1, 10): "I - NHá»®NG QUY Äá»ŠNH CHUNG",
            range(10, 21): "II - ÄÃ€O Táº O Äáº I Há»ŒC",
            range(21, 27): "III - ÄÃ€O Táº O Ká»¸ SÆ¯",
            range(27, 36): "IV - ÄÃ€O Táº O THáº C SÄ¨",
            range(36, 46): "V - ÄÃ€O Táº O TIáº¾N SÄ¨",
            range(46, 48): "VI - Tá»” CHá»¨C THá»°C HIá»†N",
        }

    def get_chuong(self, dieu_num: int) -> str:
        """Láº¥y tÃªn chÆ°Æ¡ng tá»« sá»‘ Ä‘iá»u"""
        mapping = self.extract_chuong_mapping()
        for dieu_range, chuong in mapping.items():
            if dieu_num in dieu_range:
                return chuong
        return "Unknown"

    def extract_keywords(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t keywords tá»« text"""
        keywords_dict = {
            "tÃ­n chá»‰": ["tÃ­n chá»‰", "tc", "credit"],
            "Ä‘iá»ƒm": ["Ä‘iá»ƒm", "gpa", "cpa", "grade"],
            "tá»‘t nghiá»‡p": ["tá»‘t nghiá»‡p", "graduation", "báº±ng"],
            "há»c phÃ­": ["há»c phÃ­", "tuition", "phÃ­"],
            "Ä‘Äƒng kÃ½": ["Ä‘Äƒng kÃ½", "registration"],
            "thi": ["thi", "exam", "kiá»ƒm tra"],
            "luáº­n vÄƒn": ["luáº­n vÄƒn", "thesis"],
            "luáº­n Ã¡n": ["luáº­n Ã¡n", "dissertation"],
            "Ä‘á»“ Ã¡n": ["Ä‘á»“ Ã¡n", "project"],
            "tháº¡c sÄ©": ["tháº¡c sÄ©", "master"],
            "tiáº¿n sÄ©": ["tiáº¿n sÄ©", "phd", "ncs"],
            "sinh viÃªn": ["sinh viÃªn", "student"],
            "há»c viÃªn": ["há»c viÃªn"],
            "cáº£nh bÃ¡o": ["cáº£nh bÃ¡o", "warning"],
            "thÃ´i há»c": ["thÃ´i há»c", "buá»™c thÃ´i há»c"],
        }

        found_keywords = []
        text_lower = text.lower()
        for category, variants in keywords_dict.items():
            if any(variant in text_lower for variant in variants):
                found_keywords.append(category)

        return found_keywords

    def extract_applies_to(self, text: str) -> List[str]:
        """XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng Ã¡p dá»¥ng"""
        applies = []
        text_lower = text.lower()

        if any(
            term in text_lower for term in ["sinh viÃªn", "Ä‘áº¡i há»c", "cá»­ nhÃ¢n"]
        ):
            applies.append("sinh viÃªn Ä‘áº¡i há»c")
        if any(term in text_lower for term in ["há»c viÃªn", "tháº¡c sÄ©"]):
            applies.append("há»c viÃªn tháº¡c sÄ©")
        if any(term in text_lower for term in ["ká»¹ sÆ°"]):
            applies.append("há»c viÃªn ká»¹ sÆ°")
        if any(
            term in text_lower for term in ["ncs", "nghiÃªn cá»©u sinh", "tiáº¿n sÄ©"]
        ):
            applies.append("nghiÃªn cá»©u sinh")
        if not applies:
            applies.append("táº¥t cáº£")

        return applies

    def extract_tables(self, text: str) -> Optional[str]:
        """TrÃ­ch xuáº¥t vÃ  format báº£ng biá»ƒu"""
        # Pattern cho báº£ng Ä‘iá»ƒm
        if "Ä‘iá»ƒm há»c pháº§n theo" in text.lower() and "Ä‘iá»ƒm chá»¯" in text.lower():
            return "Báº¢NG QUY Äá»”I ÄIá»‚M"

        # Pattern cho báº£ng xáº¿p loáº¡i
        if "xáº¿p loáº¡i" in text.lower() and (
            "gpa" in text.lower() or "cpa" in text.lower()
        ):
            return "Báº¢NG Xáº¾P LOáº I Há»ŒC Lá»°C"

        # Pattern cho báº£ng thá»i gian
        if "thá»i gian" in text.lower() and "khá»‘i lÆ°á»£ng" in text.lower():
            return "Báº¢NG THá»œI GIAN VÃ€ KHá»I LÆ¯á»¢NG Há»ŒC Táº¬P"

        return None

    def extract_cross_references(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t cÃ¡c tham chiáº¿u chÃ©o"""
        refs = []

        # Pattern: "táº¡i khoáº£n X Äiá»u Y"
        pattern1 = r"táº¡i\s+khoáº£n\s+(\d+)\s+Äiá»u\s+(\d+)"
        refs.extend(
            [f"Äiá»u {m[1]}, Khoáº£n {m[0]}" for m in re.findall(pattern1, text)]
        )

        # Pattern: "theo quy Ä‘á»‹nh táº¡i Äiá»u X"
        pattern2 = r"theo.*?quy Ä‘á»‹nh.*?Äiá»u\s+(\d+)"
        refs.extend([f"Äiá»u {m}" for m in re.findall(pattern2, text)])

        # Pattern: "quy Ä‘á»‹nh táº¡i Ä‘iá»ƒm X khoáº£n Y Äiá»u Z"
        pattern3 = r"Ä‘iá»ƒm\s+([a-z])\s+khoáº£n\s+(\d+)\s+Äiá»u\s+(\d+)"
        refs.extend(
            [
                f"Äiá»u {m[2]}, Khoáº£n {m[1]}, Äiá»ƒm {m[0]}"
                for m in re.findall(pattern3, text)
            ]
        )

        return list(set(refs))

    def chunk_by_dieu_khoan(self) -> List[Chunk]:
        """Chunk chÃ­nh theo Äiá»u vÃ  Khoáº£n"""
        chunks = []

        # Pattern Ä‘á»ƒ tÃ¡ch theo Äiá»u
        dieu_pattern = r"Äiá»u\s+(\d+)\.\s+([^\n]+)\n(.*?)(?=Äiá»u\s+\d+\.|CHÆ¯Æ NG\s+[IVX]+|$)"

        for match in re.finditer(
            dieu_pattern, self.text, re.DOTALL | re.IGNORECASE
        ):
            dieu_num = int(match.group(1))
            dieu_title = match.group(2).strip()
            content = match.group(3).strip()
            content = re.sub(r"\s+", " ", content).strip()
            # Láº¥y chÆ°Æ¡ng
            chuong = self.get_chuong(dieu_num)

            # TÃ¡ch theo khoáº£n
            khoan_pattern = r"(\d+)\.\s*(.*?)(?=\s*\d+\.\s*|$)"
            khoans = re.findall(
                khoan_pattern, content, re.MULTILINE | re.DOTALL
            )

            if khoans:
                # CÃ³ khoáº£n - táº¡o chunk cho tá»«ng khoáº£n
                for khoan_num, khoan_content in khoans:
                    khoan_text = khoan_content.strip()

                    # TÃ¡ch theo Ä‘iá»ƒm (a, b, c...)
                    diem_pattern = r"^([a-z]|Ä‘)\)\s+(.*?)(?=\n[a-z]\)|$)"
                    diems = re.findall(
                        diem_pattern, khoan_text, re.MULTILINE | re.DOTALL
                    )

                    if diems and len(khoan_text) > 5000:
                        # Náº¿u khoáº£n quÃ¡ dÃ i vÃ  cÃ³ nhiá»u Ä‘iá»ƒm, tÃ¡ch theo Ä‘iá»ƒm
                        for diem_char, diem_content in diems:
                            chunk_id = f"d{dieu_num}_k{khoan_num}_p{diem_char}"
                            full_text = f"""Äiá»u {dieu_num}: {dieu_title}
                                            Khoáº£n {khoan_num}, Äiá»ƒm {diem_char}:
                                            {diem_content.strip()}

                                            [ChÆ°Æ¡ng: {chuong}]"""

                            metadata = {
                                "dieu": dieu_num,
                                "khoan": int(khoan_num),
                                "diem": diem_char,
                                "chuong": chuong,
                                "title": dieu_title,
                                "keywords": self.extract_keywords(full_text),
                                "applies_to": self.extract_applies_to(
                                    full_text
                                ),
                                "has_table": self.extract_tables(full_text),
                                "references": self.extract_cross_references(
                                    full_text
                                ),
                            }

                            chunks.append(Chunk(chunk_id, full_text, metadata))
                    else:
                        # Chunk theo khoáº£n
                        chunk_id = f"d{dieu_num}_k{khoan_num}"
                        full_text = f"""Äiá»u {dieu_num}: {dieu_title}
                                        Khoáº£n {khoan_num}:
                                        {khoan_text}

                                        [ChÆ°Æ¡ng: {chuong}]"""

                        metadata = {
                            "dieu": dieu_num,
                            "khoan": int(khoan_num),
                            "chuong": chuong,
                            "title": dieu_title,
                            "keywords": self.extract_keywords(full_text),
                            "applies_to": self.extract_applies_to(full_text),
                            "has_table": self.extract_tables(full_text),
                            "references": self.extract_cross_references(
                                full_text
                            ),
                        }

                        chunks.append(Chunk(chunk_id, full_text, metadata))
            else:
                # KhÃ´ng cÃ³ khoáº£n - chunk toÃ n bá»™ Ä‘iá»u
                chunk_id = f"d{dieu_num}"
                full_text = f"""Äiá»u {dieu_num}: {dieu_title}
{content}

[ChÆ°Æ¡ng: {chuong}]"""

                metadata = {
                    "dieu": dieu_num,
                    "chuong": chuong,
                    "title": dieu_title,
                    "keywords": self.extract_keywords(full_text),
                    "applies_to": self.extract_applies_to(full_text),
                    "has_table": self.extract_tables(full_text),
                    "references": self.extract_cross_references(full_text),
                }

                chunks.append(Chunk(chunk_id, full_text, metadata))

        return chunks

    def extract_special_tables(self) -> List[Chunk]:
        """TrÃ­ch xuáº¥t cÃ¡c báº£ng quan trá»ng thÃ nh chunks riÃªng"""
        chunks = []

        # Báº£ng quy Ä‘á»•i Ä‘iá»ƒm
        if "Äiá»ƒm há»c pháº§n theo" in self.text:
            table_text = """Báº¢NG QUY Äá»”I ÄIá»‚M Há»ŒC PHáº¦N (Äiá»u 5, Khoáº£n 6)

Thang 10 â†’ Äiá»ƒm chá»¯ â†’ Thang 4:
- 9.5-10.0 â†’ A+ â†’ 4.0
- 8.5-9.4 â†’ A â†’ 4.0
- 8.0-8.4 â†’ B+ â†’ 3.5
- 7.0-7.9 â†’ B â†’ 3.0
- 6.5-6.9 â†’ C+ â†’ 2.5
- 6.0-6.4 â†’ C â†’ 2.0
- 5.5-5.9 â†’ D+ â†’ 1.5
- 5.0-5.4 â†’ D â†’ 1.0
- 4.0-4.9 â†’ D â†’ 1.0
- 0.0-3.9 â†’ F â†’ 0

LÆ¯U Ã:
- Äiá»ƒm Ä‘áº¡t: tá»« D trá»Ÿ lÃªn (â‰¥4.0)
- Há»c pháº§n tá»‘t nghiá»‡p: pháº£i tá»« C trá»Ÿ lÃªn (â‰¥5.0)
- Äiá»ƒm liá»‡t: <5 vá»›i Ä‘á»“ Ã¡n/khÃ³a luáº­n tá»‘t nghiá»‡p, <3 vá»›i há»c pháº§n khÃ¡c"""

            chunks.append(
                Chunk(
                    "table_diem_quy_doi",
                    table_text,
                    {
                        "type": "table",
                        "table_name": "quy_doi_diem",
                        "dieu": 5,
                        "khoan": 6,
                        "keywords": ["Ä‘iá»ƒm", "quy Ä‘á»•i", "gpa"],
                        "applies_to": ["táº¥t cáº£"],
                    },
                )
            )

        # Báº£ng xáº¿p loáº¡i há»c lá»±c
        if "Xáº¿p loáº¡i" in self.text and "GPA hoáº·c CPA" in self.text:
            table_text = """Báº¢NG Xáº¾P LOáº I Há»ŒC Lá»°C (Äiá»u 12, Khoáº£n 6)

GPA/CPA â†’ Xáº¿p loáº¡i:
- 3.60-4.00 â†’ Xuáº¥t sáº¯c
- 3.20-3.59 â†’ Giá»i
- 2.50-3.19 â†’ KhÃ¡
- 2.00-2.49 â†’ Trung bÃ¬nh
- 1.00-1.99 â†’ Yáº¿u
- <1.00 â†’ KÃ©m

Ãp dá»¥ng cho:
- GPA: Xáº¿p loáº¡i há»c lá»±c theo há»c ká»³
- CPA: Xáº¿p loáº¡i há»c lá»±c tá»« Ä‘áº§u khÃ³a vÃ  xáº¿p háº¡ng tá»‘t nghiá»‡p"""

            chunks.append(
                Chunk(
                    "table_xep_loai",
                    table_text,
                    {
                        "type": "table",
                        "table_name": "xep_loai_hoc_luc",
                        "dieu": 12,
                        "khoan": 6,
                        "keywords": ["Ä‘iá»ƒm", "xáº¿p loáº¡i", "gpa", "cpa"],
                        "applies_to": ["táº¥t cáº£"],
                    },
                )
            )

        # Báº£ng trÃ¬nh Ä‘á»™ nÄƒm há»c
        if "Sá»‘ TCTL" in self.text and "TrÃ¬nh Ä‘á»™" in self.text:
            table_text = """Báº¢NG TRÃŒNH Äá»˜ NÄ‚M Há»ŒC (Äiá»u 12, Khoáº£n 5)

Sá»‘ TC tÃ­ch lÅ©y â†’ TrÃ¬nh Ä‘á»™ nÄƒm há»c:
- <32 TC â†’ NÄƒm thá»© nháº¥t
- 32-63 TC â†’ NÄƒm thá»© hai
- 64-95 TC â†’ NÄƒm thá»© ba
- 96-127 TC â†’ NÄƒm thá»© tÆ°
- â‰¥128 TC â†’ NÄƒm thá»© nÄƒm

Ãp dá»¥ng: Sinh viÃªn Ä‘áº¡i há»c"""

            chunks.append(
                Chunk(
                    "table_trinh_do_nam",
                    table_text,
                    {
                        "type": "table",
                        "table_name": "trinh_do_nam_hoc",
                        "dieu": 12,
                        "khoan": 5,
                        "keywords": ["tÃ­n chá»‰", "nÄƒm há»c"],
                        "applies_to": ["sinh viÃªn Ä‘áº¡i há»c"],
                    },
                )
            )

        return chunks

    def extract_all(self) -> List[Chunk]:
        """TrÃ­ch xuáº¥t táº¥t cáº£ chunks"""
        chunks = []

        # 1. Chunks chÃ­nh theo Ä‘iá»u/khoáº£n
        chunks.extend(self.chunk_by_dieu_khoan())

        # 2. Báº£ng biá»ƒu Ä‘áº·c biá»‡t
        chunks.extend(self.extract_special_tables())

        self.chunks = chunks
        return chunks

    def save_to_json(self, filepath: str):
        """LÆ°u chunks ra file JSON"""
        data = {
            "metadata": {
                "document": "Quy cháº¿ Ä‘Ã o táº¡o ÄHBK HÃ  Ná»™i",
                "version": "QÄ 4600/QÄ-ÄHBK ngÃ y 09/06/2023",
                "total_chunks": len(self.chunks),
                "created_at": datetime.now().isoformat(),
            },
            "chunks": [chunk.to_dict() for chunk in self.chunks],
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ÄÃ£ lÆ°u {len(self.chunks)} chunks vÃ o {filepath}")

    def get_statistics(self) -> Dict:
        """Thá»‘ng kÃª vá» chunks"""
        stats = {
            "total_chunks": len(self.chunks),
            "by_chuong": {},
            "by_keywords": {},
            "by_applies_to": {},
            "with_tables": 0,
            "with_references": 0,
        }

        for chunk in self.chunks:
            # By chÆ°Æ¡ng
            chuong = chunk.metadata.get("chuong", "Unknown")
            stats["by_chuong"][chuong] = stats["by_chuong"].get(chuong, 0) + 1

            # By keywords
            for kw in chunk.metadata.get("keywords", []):
                stats["by_keywords"][kw] = stats["by_keywords"].get(kw, 0) + 1

            # By applies_to
            for applies in chunk.metadata.get("applies_to", []):
                stats["by_applies_to"][applies] = (
                    stats["by_applies_to"].get(applies, 0) + 1
                )

            # Tables
            if chunk.metadata.get("has_table"):
                stats["with_tables"] += 1

            # References
            if chunk.metadata.get("references"):
                stats["with_references"] += 1

        return stats


# ====================== MAIN USAGE ======================


def main(text_content: str):
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y extraction"""

    print("ğŸš€ Báº¯t Ä‘áº§u extract dá»¯ liá»‡u RAG tá»« Quy cháº¿ Ä‘Ã o táº¡o ÄHBK...")

    # Khá»Ÿi táº¡o extractor
    extractor = QuyCheDaoTaoExtractor(text_content)

    # Extract táº¥t cáº£ chunks
    chunks = extractor.extract_all()

    # Thá»‘ng kÃª
    stats = extractor.get_statistics()
    print(f"\nğŸ“Š THá»NG KÃŠ:")
    print(f"  - Tá»•ng sá»‘ chunks: {stats['total_chunks']}")
    print(f"  - Chunks cÃ³ báº£ng: {stats['with_tables']}")
    print(f"  - Chunks cÃ³ tham chiáº¿u: {stats['with_references']}")

    print(f"\nğŸ“š PhÃ¢n bá»‘ theo chÆ°Æ¡ng:")
    for chuong, count in sorted(stats["by_chuong"].items()):
        print(f"  - {chuong}: {count} chunks")

    print(f"\nğŸ”‘ Top keywords:")
    top_keywords = sorted(
        stats["by_keywords"].items(), key=lambda x: x[1], reverse=True
    )[:10]
    for kw, count in top_keywords:
        print(f"  - {kw}: {count} chunks")

    # LÆ°u ra file
    extractor.save_to_json("quy_che_rag_data.json")

    # In má»™t vÃ i chunks máº«u
    print(f"\nğŸ“ MáºªU CHUNKS:")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"ID: {chunk.id}")
        print(f"Text (100 kÃ½ tá»± Ä‘áº§u): {chunk.text[:100]}...")
        print(
            f"Metadata: {json.dumps(chunk.metadata, ensure_ascii=False, indent=2)}"
        )

    return extractor, chunks


# ====================== PDF EXTRACTION ======================


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text tá»« PDF file"""
    try:
        import PyPDF2

        print(f"ğŸ“„ Äang Ä‘á»c PDF: {pdf_path}")
        text = ""

        with open(pdf_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            print(f"   Tá»•ng sá»‘ trang: {total_pages}")

            for i, page in enumerate(pdf_reader.pages, 1):
                page_text = page.extract_text()
                text += page_text + "\n"
                if i % 10 == 0:
                    print(f"   ÄÃ£ xá»­ lÃ½ {i}/{total_pages} trang...")

        print(f"âœ… HoÃ n thÃ nh! Tá»•ng {len(text)} kÃ½ tá»±\n")
        return text

    except ImportError:
        print("âŒ Cáº§n cÃ i Ä‘áº·t PyPDF2: pip install PyPDF2")
        return None
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c PDF: {e}")
        return None


def extract_text_from_pdf_advanced(pdf_path: str) -> str:
    """Extract text tá»« PDF vá»›i pdfplumber (tá»‘t hÆ¡n cho tiáº¿ng Viá»‡t)"""
    try:
        import pdfplumber

        print(f"ğŸ“„ Äang Ä‘á»c PDF vá»›i pdfplumber: {pdf_path}")
        text = ""

        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"   Tá»•ng sá»‘ trang: {total_pages}")

            for i, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                if i % 10 == 0:
                    print(f"   ÄÃ£ xá»­ lÃ½ {i}/{total_pages} trang...")

        print(f"âœ… HoÃ n thÃ nh! Tá»•ng {len(text)} kÃ½ tá»±\n")
        return text

    except ImportError:
        print("âŒ Cáº§n cÃ i Ä‘áº·t pdfplumber: pip install pdfplumber")
        print("   Äang fallback sang PyPDF2...")
        return extract_text_from_pdf(pdf_path)
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c PDF: {e}")
        return None


# ====================== MAIN USAGE ======================

if __name__ == "__main__":
    import sys

    # CÃ¡ch 1: Äá»c tá»« file PDF
    pdf_file = "QCDT-2023.pdf"  # Thay Ä‘á»•i tÃªn file náº¿u cáº§n

    print("=" * 60)
    print("ğŸš€ RAG DATA EXTRACTION - ÄHBK QUY CHáº¾ ÄÃ€O Táº O")
    print("=" * 60 + "\n")

    # Extract text tá»« PDF
    text_content = extract_text_from_pdf_advanced(pdf_file)

    if not text_content:
        print("\nâŒ KhÃ´ng thá»ƒ Ä‘á»c file PDF. Vui lÃ²ng kiá»ƒm tra:")
        print("   1. File PDF tá»“n táº¡i: QCDT-2023-upload.pdf")
        print("   2. ÄÃ£ cÃ i Ä‘áº·t: pip install pdfplumber PyPDF2")
        sys.exit(1)

    # Cháº¡y extraction
    extractor, chunks = main(text_content)

    print("\n" + "=" * 60)
    print("âœ… HOÃ€N THÃ€NH!")
    print("=" * 60)
    print(f"ğŸ“ File output: quy_che_rag_data.json")
    print(f"ğŸ“Š Tá»•ng chunks: {len(chunks)}")
    print("\nğŸ’¡ Tiáº¿p theo:")
    print("   - Load JSON vÃ o vector database")
    print("   - Táº¡o embeddings cho tá»«ng chunk")
    print("   - Build RAG pipeline vá»›i LangChain/LlamaIndex")
